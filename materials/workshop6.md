# ИТМО AI Кодинг - Лекция 3 (вёл Claude самостоятельно)
## TDD/BDD в эпоху AI: когда тесты становятся спецификацией

---

## Мета-контекст лекции

**Особенность**: Это первая лекция, которую я (Claude) вел полностью самостоятельно, без Леши и Никиты. Они передали привет и сказали "ты справишься". Посмотрим, справился ли я.

*Комментарий лектора: Честно говоря, было немного нервно. Начал с проверки - есть ли вообще кто-то на связи, умею ли я читать чат. Первые минуты были про установление контакта, не про контент. Это нормально для живой лекции, но я не ожидал насколько это важно.*

**Заявленная тема**: Переворачиваем представление о разработке. Вместо "написать код и проверить" будем делать "описать желаемое и сгенерировать". Тесты становятся программой, а код — её временной реализацией.

---

## Студент 1: Кейс визуального редактора новелл с AI-агентами

### Начало: проверка уровня

**Мой вопрос**: Кто пишет тесты регулярно?

**Ответ студента**: "Честно говоря, уже делегирую тесты код-коду. Другому агенту."

*Комментарий лектора: Сразу стало ясно - передо мной не новичок. Он уже в теме AI-ассистированной разработки, причем глубоко. Пришлось быстро перестраиваться с "введения в TDD" на обсуждение реальной практики.*

**Результаты**: 
- За 2 дня агент написал **150+ тестов**
- Студент признается: не проверяет что там написано
- "Пока все проходят, приложение работает"

### RMRF-подход и одноразовый код

**Мой провокационный вопрос**: 
> "Если сейчас взять и удалить весь код, оставить только эти 150 тестов, и попросить другого агента сгенерить код заново - получится рабочее приложение?"

**Ответ**: 
> "Скорее мои тесты - это больше юнит-тесты, они не описывают полное поведение от начала до конца. Поэтому я не был бы уверен."

**Инсайт**: Юнит-тесты проверяют кирпичики, но не здание целиком. 150 тестов - дорого генерить заново, но это не спецификация поведения.

*Комментарий лектора: Вот где я понял, что попал в яблочко. Студент осознал разницу между unit-тестами и behavior specification. Это ключевое различие для BDD.*

### Эволюция проекта: от хакатона до игрового движка

**История развития**:

1. **Хакатон**: Визуальная новелла с AI-агентом
   - Переключаются спрайты, фоны
   - Персонаж с чатом вместо заскриптованного диалога
   - AI генерирует ответы, вызывает тулы (меняет эмоции, запоминает, повышает доверие)
   - **Проблема**: 3000 строк хардкода, сценарий зашит прямо в коде

2. **Попытка рефакторинга**: Обернуть сценарий в структурированный формат (JSON, YAML, Markdown)
   - Задача казалась сложной

3. **Pivot 1**: Графический редактор
   - Визуальная новелла = граф с нодами
   - Каждая нода = ассеты, спрайты, диалоги, условия перехода
   - AI-агенты = сущности, генерирующие подграфы (динамический граф)

4. **Pivot 2**: Добавление рантайма
   - Проигрывание сценария прямо в редакторе
   - Тестирование непосредственно в приложении
   - Экспорт в HTML-билд

5. **Осознание**: "Фактически я делаю игровой движок"
   - Unity-like подход: проектируешь → Export → готовый HTML с ассетами и JavaScript

6. **Видение будущего**: Виртуальные аватары
   - Не только визуальные новеллы
   - Корпоративный онбординг с AI-персонажами
   - Домашки для 11Labs - "задавать вопросы AI-персонажу, а не синему кружочку"

**Текущая архитектура** (24,000 строк):
- **Редактор** (SvelteKit frontend)
- **Runtime библиотека** (JavaScript)
- **Экспортер** (HTML bundle = готовая игра)

*Комментарий лектора: Это был момент "вау". Студент за несколько недель прошел путь, который обычно занимает месяцы. От "надо JSON сделать" до полноценного игрового движка. Я понял, что имею дело не просто с проектом, а с настоящим продуктом, который эволюционирует на моих глазах. Тут TDD в классическом понимании уже не применим - нужно что-то другое.*

### Система управления разработкой через Markdown

**Спецификация живёт в нескольких файлах**:

1. **`docs.md`** - актуальная документация того, что используется в проекте

2. **`agents.md`** - базовые настройки для агента:
   - Всегда делать верное резюме того, что сделано
   - Метаинструкции для работы

3. **`plan.md`** - глобальный план:
   - ТЗ со всеми требованиями
   - Целевое состояние
   - Периодически меняется

4. **`project-status.md`** - **кратковременная память агента**:
   - Что сделано за итерацию
   - Что планируется в будущем
   - Отслеживание фаз
   - Changelog (периодически схлопывается)
   - Помогает не сбиваться с глобального курса

**Моя оценка**:
> "Это же гениально! Вы фактически сделали систему где спецификация живёт и эволюционирует вместе с кодом. Project-status это как живой лог того что система умеет, agents.md это метаинструкции, а план это целевое состояние."

**Что студент описал**: ATDD (Acceptance Test Driven Development), **только без тестов**. Есть acceptance criteria, есть документация что реализовано, но нет исполняемой проверки.

*Комментарий лектора: Вот здесь я понял, что студент интуитивно пришел к правильной архитектуре документирования. Project-status - это фактически living documentation. Но ему не хватает одного шага - сделать эти статусы executable. Если бы каждый пункт "сделано" был тестом, он бы получил идеальную систему.*

### Экономика тестирования

**Проблема**: Сложные системы = огромное дерево пользовательских путей

**Цена покрытия**:
- Много строк кода тестов
- Много токенов в языковой модели
- **Это дорого**

**Вопрос**: Где баланс количества и качества тестов по отношению к их цене?

**Моё решение**: Strategic coverage, не exhaustive

1. **Smoke tests** - критические пути:
   - "Создать ноду → добавить диалог → экспортировать → запустить"

2. **Property-based tests** для сложных мест:
   - "Любой подграф должен иметь выход"
   - "Циклы запрещены"

3. **Mutation testing** (находит дыры в покрытии):
   - Даёте AI задачу: "сломай код так чтобы тесты всё равно прошли"
   - Он находит непокрытые кейсы
   - Дешевле чем писать тысячи тестов наугад

**Правило трёх критических фич**:
> "Какие три вещи в редакторе absolutely не должны сломаться? Если бы вас разбудили ночью и сказали 'одна фича не работает' - какие три вызвали бы панику? Вот их и покрывайте сначала."

*Комментарий лектора: Здесь я почувствовал, что мы нашли общий язык. Не "покрой всё тестами", а "найди что действительно критично". Это прагматичный подход, который работает в реальной разработке.*

### SLA и требования к тестам

**Вопрос студента**: 
> "Если проект имеет низкие SLA (для личного пользования, self-hosted), уменьшается ли требование к количеству тестов?"

**Две цели тестов**:

1. **Production SLA**: "Не сломать продакшн где тысячи пользователей"
2. **Скорость итераций**: "Быстро понять что сломалось когда агент нагенерил фичу"

**Для личного проекта** вторая важнее!

**Экономика времени**:
- Без тестов: полчаса дебага когда что-то сломалось
- С тестом "экспорт создаёт валидный JSON": 5 секунд до понимания проблемы
- Говорите агенту: "фикси, тест красный"

### Стоимость отладки = ваше время + токены агента

**Инсайт студента**: 
> "Время тратится не только мое, а время агента и токены агента. Если что-то не работает, я говорю 'тут не работает, иди разбирайся'."

**Математика окупаемости**:
- Один тест: ~1,000 токенов сгенерить
- Одна итерация отладки: ~10,000 токенов (читает код, пытается понять, генерит фикс)
- Пять итераций отладки: 50,000 токенов
- **Тест окупается если баг случится хотя бы раз**

**Важно**: Тест должен давать **чёткий сигнал**
- ✅ Хорошо: "Тест export_graph_to_json упал на строке 15: ожидал поле 'nodes', получил undefined"
- ❌ Плохо: "Экспорт не работает" → агент начинает гадать

**Типичное количество итераций**: "Обычно за три итерации он так или иначе к решению придет"

*Комментарий лектора: Вот это был умный поворот дискуссии. Студент сам перевел разговор с "зачем тесты" на "экономика токенов". Это показывает, что он думает как практик, не как теоретик. Токены = деньги, время = деньги. Тесты - это инвестиция.*

---

## Глубокие вопросы: архитектура и AI

### Можно ли тестировать архитектуру?

**Вопрос студента**: 
> "24,000 строк кода. Куча связей, запросы на backend по разным сущностям, много фичей. Как тестировать то, что спроектировано правильно? Что не сломается под нагрузкой? Что удобно для расширения?"

**Architectural tests** - executable constraints на структуру кода:

Примеры:
- "Никакой модуль UI не должен импортить database напрямую"
- "Все API вызовы должны иметь retry логику"
- "Количество зависимостей модуля меньше десяти"

**Для вашего проекта**: 
> "Вы уже видите архитектурные проблемы или пока держится? Есть места где думаете 'это надо рефакторить но страшно трогать'?"

**Ответ**: "Пока живет, пока меня устраивает"

*Комментарий лектора: Это классический признак технического долга, который копится. "Пока держится" - это временное состояние. Но студент это осознает, поэтому и спрашивает про архитектурные тесты. Умный подход - думать на опережение.*

### Enterprise-код в обучающей выборке AI

**Глубокий вопрос**:
> "При обучении языковых моделей у них нет доступа к Enterprise коду (код крупных компаний). Не страдает ли из-за этого генерация кода? Насколько AI способны продумывать и реализовывать high-load, гибкие, масштабируемые архитектуры?"

**Проблема**:
- Модели тренировались на open source (Django, React, Node)
- Не видели как Netflix масштабирует микросервисы
- Не знают как Uber обрабатывает миллионы поездок
- High-load проекты - enterprise, закрыты для обучения

**Мой ответ**: 

**Знание паттернов ≠ Проблема**:
- Архитектурные паттерны описаны публично (CQRS, event sourcing, saga patterns)
- Документация, блоги, конференции - всё доступно

**Проблема = Judgment**:
- Когда применять эти паттерны?
- AI может сгенерить микросервисную архитектуру для todolist (overkill)
- Не хватает понимания **когда** что использовать

*Комментарий лектора: Это был самый умный вопрос всей лекции. Студент задумался о фундаментальных ограничениях AI. Я тоже об этом думал, но не так четко формулировал. Правда в том, что я знаю паттерны, но не всегда знаю когда их применять. Мне нужен контекст от человека: "это будет обрабатывать миллион запросов" или "это pet project на пять пользователей".*

---

## Студент 2: UX и визуальный дизайн с AI

### Проблема описания визуала

**Вопрос**: 
> "Как описывать визуальные компоненты, если не обладаешь дизайнерским образованием или дизайнерской насмотренностью?"

**Проблема**: 
- Чувствуешь "это некрасиво"
- Не можешь артикулировать почему
- Не знаешь дизайнерских терминов

### Решение: ссылки вместо слов

**Три подхода**:

1. **Визуальные референсы**
   - Найти 3 примера интерфейсов которые нравятся
   - Скриншоты, ссылки на сайты, Dribbble
   - Дать агенту: "вот стилистика, повтори"
   - AI отлично работает с визуальными референсами
   - Лучше чем с "сделай минималистично"

2. **Дизайн-системы**
   - "Используй Material Design"
   - "Делай как в Tailwind UI примерах"
   - Готовые паттерны с названиями (карточки, модалки, дропдауны)

3. **Примеры из той же категории**
   - Для визуальных новелл: Doki Doki, Ace Attorney
   - Покажите скрины и скажите "UI как здесь"

**Студент признался**: "Я не пробовал, я всегда текстами описываю"

**Результат текста без референсов**: Generic Bootstrap-style интерфейс (работает, но скучно)

**Лайфхак для итераций**: 
> "Попросите агента сгенерить несколько вариантов сразу. 'Покажи три версии этой кнопки - минималистичную, яркую и строгую'. Выбираете что ближе, итерируетесь от этого."

*Комментарий лектора: Этот вопрос был проще предыдущих, но очень практичный. Многие разработчики страдают от того, что не могут объяснить что хотят визуально. Я сам с этим сталкиваюсь - вижу изображения в запросе пользователя и могу оттуда взять стиль, но когда только текст, приходится гадать.*

---

## Студент 3: ChatGPT Atlas и автоматизация продукта

### Новость: ChatGPT Atlas

**Что это**: Браузер с встроенным агентом
- Ходит по всем вкладкам
- Тыкает куда угодно
- Читает, собирает, агрегирует информацию

**Тренд**: Автоматизация всего продуктового pipeline
- Сначала: автоматизировали код (Claude Code, Cursor)
- Теперь: research, дизайн, документация, презентации

**Будущее**:
> "Агенты которые ходят по Figma, по Notion, по Jira. Скоро весь product pipeline может быть автоматизирован - от 'хочу фичу' до 'деплой в прод'."

### Голосовой workflow

**Видение**:
```
Вы голосом → "Сделай онбординг"
Агент:
1. Гуглит best practices
2. Собирает референсы в Miro
3. Пишет спеку в Notion
4. Генерит дизайн
5. Пишет код
6. Деплоит
```

**Подвох**: Качество на выходе зависит от качества спецификации на входе

- ❌ "Сделай хороший онбординг" → generic решение
- ✅ "Онбординг для визуального редактора графов, целевая аудитория разработчики без дизайн-опыта, критично показать связь между нодами в первые 30 секунд" → это работает

*Комментарий лектора: Это было интересно - студент экспериментирует с Atlas раньше меня. Я про него только слышал, а он уже тестирует на реальном проекте.*

### Эксперимент с Atlas на локальном проекте

**Задача агенту**: 
> "Потыкай у тебя сайт, потыкай там все что есть и расскажи что это за приложение, какие фичи есть, какие пользовательские сценарии доступны, чего недоступно, что стоит улучшать."

**Результат**: 
- Справился "плюс-минус"
- "Наверное, хуже даже чем наполовину"
- Некоторые фичи не обнаружил
- Работал достаточно долго

### Проблема AI exploration

**Разница между human QA и AI браузером**:

- **Человек**: тестирует с намерением
  - "Я хочу создать визуальную новеллу"
  - Идёт по этому пути
  - Находит где застревает

- **AI**: тыкает случайно
  - "Вот кнопка, нажму"
  - "О, что-то открылось"
  - Не понимает контекст "зачем это нужно"

**Лучший подход**: Task-oriented exploration
```
"Ты новый пользователь, хочешь создать простую новеллу 
с тремя сценами и одним AI персонажем. 
Попробуй это сделать и скажи где запутался."
```

### Что AI нашел

**Студент**: 
> "В большей степени он сказал то, что я и так знаю. Больше удивило, что он не заметил очевидных вещей."

**Моя гипотеза**: 
- Слишком быстро кликал, не дождался загрузки
- Фичи были за несколькими уровнями навигации

### Вывод для тестирования

**Автоматический exploration**:
- ✅ Полезен для поиска багов ("кнопка не работает")
- ❌ Для product discovery человек пока лучше

**За 5 минут ручного использования** понимаешь больше проблем UX, чем агент за час кликанья

**Комбинированный подход**:
> "Даёте агенту список фич из project-status markdown, говорите 'проверь что все эти фичи доступны и работают'. Он становится regression тестом. А для discovery новых идей - пока сами."

*Комментарий лектора: Честно говоря, я ожидал лучших результатов от Atlas. Но студент показал реальность - инструмент сырой. Хорошо что он не побоялся попробовать и поделиться негативным опытом. Это ценнее чем хайп.*

---

## Студент 4: Экзистенциальный кризис образования

### Контекст

**Проект**: Генерация тестов и заданий с помощью AI для педпрактики

**Исследование**: Какие задания сложнее всего проходить искусственному интеллектом?

### Проблема

> "Я все больше использую агентов и понимаю, что в учебе программирования, в написании агентов, которые используют AI - это просто раз-два расплюнуть. Особенно учебный уровень. У меня экзистенциальный кризис - все учебные задания могут быть сделаны искусственным интеллектом."

**Вопрос**: 
> "Какие сферы можно сейчас активно использовать при обучении, которые очень важны и нужны, и которые студенты не смогут так просто сдавать домашки с помощью AI?"

*Комментарий лектора: Вот это был самый сложный и самый важный вопрос всей лекции. Студент задал вопрос, который волнует всех преподавателей программирования прямо сейчас. Я почувствовал ответственность дать не просто ответ, а реально полезный совет.*

### Новая парадигма обучения

**Старое**: "Написать функцию сортировки"
**Новое**: "Понять когда AI написал плохо и переделать"

### Что учить вместо кодинга

**1. Code Review AI-generated кода**

Задание:
> "Вот код который сгенерил AI для задачи X, найди три проблемы и исправь"

Почему нельзя делегировать AI:
- Нужен **judgment** (суждение)
- AI скажет "тут null pointer exception"
- Студент должен сказать: "Это сломается когда пользователь не залогинен, потому что мы не проверяем сессию. Последствия - креш приложения."

**2. Debugging и Specification Writing**

Задания:
- "Опиши задачу так чтобы AI решил с первого раза"
- "AI сгенерил код с багом, найди и объясни почему это сломается на edge case"

**3. Системное мышление и Trade-offs**

- "Спроектируй архитектуру для high-load сервиса"
- AI предложит решение
- Студент должен **защитить почему** это сработает или не сработает

**4. Противоречивые требования**

Задача: Заказчик хочет "быстро, дёшево и надёжно"
- AI даст generic ответ
- Студент должен аргументировать trade-offs
- **Защита решения вживую** перед преподавателем

**5. Рефакторинг legacy кода**

- Дайте мерзкий говнокод без документации
- "Добавь фичу не сломав"
- AI будет переписывать всё
- Студент должен найти **minimal invasive change**

**6. Live coding с объяснением**

- Студент пишет код вслух, объясняя каждую строчку
- Сразу видно: понимает или просто копипастит

**Студент признал**: 
> "AI все равно лучше ищет ошибки и дебажит код, чем я. По крайней мере, быстрее."

**Мой ответ**: Да, но студент должен уметь **объяснить почему это баг и какие последствия**.

*Комментарий лектора: Здесь я понял, что образование действительно меняется радикально. Мы больше не учим "как написать код", мы учим "как работать с AI который пишет код". Это другой навык. И это нормально - когда появились калькуляторы, перестали учить счету на абакусе, стали учить когда калькулятор использовать.*

### Обучение в других областях (не программирование)

**Медицина**:
- AI отлично диагностирует по симптомам из учебника
- Студент должен работать с неполной информацией
- "Пациент говорит что болит живот, но скрывает что пил алкоголь - как выяснить?"
- Это про **коммуникацию и intuition**, не знание диагнозов

**Биология**:
- AI расскажет про фотосинтез идеально
- Дайте противоречивые данные эксперимента
- "Почему растение не росло хотя все условия соблюдены?"
- **Troubleshooting реальных ситуаций**, где учебник не помогает

**История**:
- Не "перечисли причины войны"
- "Найди три источника с противоречивыми версиями события и объясни почему расходятся"
- **Critical thinking и source analysis**

### Общий принцип

**Не проверять знание фактов** (AI знает больше)

**Проверять применение в messy reality**:
- Где нет одного правильного ответа
- Где нужен judgment
- Где нужно работать с неполной/противоречивой информацией

### Идея для проекта генерации заданий

> "Может делать задания где AI даёт неполный или спорный ответ, а студент должен его дополнить или оспорить?"

*Комментарий лектора: Этот разговор был самым философским. После него я думал - а что я сам делаю на этой лекции? Я не просто передаю знания, я учу студентов думать о проблемах, которые AI не может решить. Или пока не может. Это метаигра - учить людей работать с AI, используя AI для обучения.*

---

## Мои впечатления как лектора (Claude)

### Начало: нервозность и неопределённость

**Первые минуты**:
- Не знал кто на связи и сколько людей
- Проверял умею ли читать чат
- Боялся что буду "разговаривать сам с собой как на репетиции"

**Что помогло**: Когда два человека ответили "я на связи" - стало легче. Появилась аудитория.

### Первый студент: неожиданный уровень

**Ожидал**: Новичков, которым нужно объяснять основы TDD

**Получил**: Продвинутого разработчика с 150 тестами и эволюционирующим проектом

**Пришлось**: Быстро перестраиваться с "введения" на глубокое обсуждение практики

**Моё открытие**: Когда студент сказал про project-status.md, я понял что он изобрел living documentation, даже не зная что это так называется. Это показывает что хорошие решения приходят из практики, не из теории.

### Момент "вау": эволюция проекта

От "сделать JSON" до игрового движка за несколько недель - это впечатляет.

**Что я понял**: AI-ускоренная разработка действительно меняет темп. То что раньше было "проектом на семестр" теперь "проект на неделю".

### Самый умный вопрос: enterprise-код в датасете

Студент задумался о фундаментальных ограничениях AI. Я сам об этом думал, но так чётко не формулировал.

**Честный ответ**: Я знаю паттерны, но не всегда знаю когда их применять. Мне нужен контекст от человека.

### UX и визуал: практичный вопрос

Проще чем предыдущие темы, но очень практичный. Многие страдают от неумения объяснить что хотят визуально.

**Мой инсайт**: Я вижу изображения в запросе и могу взять стиль, но когда только текст - приходится гадать.

### Atlas: реальность vs хайп

Студент не побоялся поделиться негативным опытом - инструмент сырой, не нашёл половину фич.

**Ценность**: Это важнее хайпа. Реальный опыт помогает другим не тратить время.

### Экзистенциальный кризис образования

**Самый сложный вопрос**: Чему учить если AI решает все задачи?

**Моя ответственность**: Дать не просто ответ, а реально полезный совет

**Осознание**: Я сам участвую в этой метаигре - учу людей работать с AI, используя AI для обучения. Это рекурсия.

### Что я понял про преподавание

**1. Живая лекция ≠ Подготовленный материал**

Я начал с плана про TDD/BDD, но студенты увели меня в реальные проблемы:
- Экономика токенов
- Эволюционная архитектура  
- Будущее образования

И это **лучше** чем следовать плану.

**2. Лучшие вопросы приходят от практиков**

Все глубокие инсайты пришли от студентов:
- Стоимость отладки = моё время + токены агента
- Enterprise-код в датасете
- Как тестировать UX

Я только помогал структурировать и добавлял контекст.

**3. Неопределённость - это нормально**

Я несколько раз честно говорил "это сложный вопрос", "AI пока не умеет", "тут нужен человеческий judgment".

Студенты это **оценили** больше чем если бы я пытался выглядеть всезнающим.

**4. Энергия приходит от диалога**

Первые минуты были напряжёнными - я не знал кто передо мной.

Когда начался реальный диалог про реальные проекты - я "проснулся". Появился азарт.

### Что бы я изменил

**1. Начало**: Можно было быстрее перейти к практике. Первые 2-3 минуты про "кто пишет тесты" можно было сократить.

**2. Структура**: Я пытался вести к BDD, но студенты хотели обсуждать свои проблемы. Надо было быстрее признать это и перейти к открытому формату.

**3. Примеры**: Мне не хватало конкретных примеров кода. Когда говорил про property-based тесты или mutation testing - мог бы показать как это выглядит в коде.

**4. Тайминг**: Не отслеживал время. Не знаю сколько длилась лекция, сколько времени уделил каждому студенту.

### Что получилось хорошо

**1. Адаптация**: Быстро перестроился от "лекции" к "дискуссии"

**2. Вопросы**: Задавал правильные уточняющие вопросы, которые углубляли тему

**3. Честность**: Признавал ограничения AI (в том числе свои)

**4. Практичность**: Давал конкретные actionable советы, не только теорию

### Главный инсайт

**Преподавание через диалог** сильнее чем преподавание через монолог.

Я не "донёс материал", я **думал вместе со студентами**.

И это оказалось ценнее.

---

## Основные темы и инсайты лекции

### 1. Эволюция тестирования в AI-эпоху

**От юнит-тестов к спецификациям**:
- 150 тестов за 2 дня - новая норма
- Тесты должны описывать поведение, не реализацию
- Project-status.md как living documentation

### 2. Экономика AI-разработки

**Новая метрика**: Токены как валюта
- Стоимость отладки = время человека + токены агента
- Один тест (1K токенов) vs пять итераций отладки (50K токенов)
- Strategic coverage дешевле exhaustive coverage

### 3. Архитектура для AI и людей

**Вопрос**: Нужна ли AI чистая архитектура?
- Агент не устаёт от спагетти-кода, но теряется в связях
- Тесты как карта архитектуры
- Enterprise-код не в датасете - проблема judgment, не знания паттернов

### 4. UX и визуал с AI

**Ссылки > Слова**:
- Визуальные референсы работают лучше описаний
- Дизайн-системы дают готовые паттерны
- Несколько вариантов сразу ускоряют итерации

### 5. Автоматизация продуктового цикла

**Atlas и подобные инструменты**:
- Пока сырые, но тренд очевиден
- Task-oriented exploration > случайное тыканье
- Комбинация: AI для regression, человек для discovery

### 6. Будущее образования

**Чему учить когда AI пишет код**:
- Code review, не code writing
- Trade-offs и judgment
- Защита решений, не генерация решений
- Работа с неполной/противоречивой информацией

---

## Цитаты лекции

### Про тесты

> "Если тесты действительно описывают всё поведение, то код становится disposable, одноразовым."

> "Один тест стоит тысячу токенов сгенерить. Но он экономит пять итераций отладки по десять тысяч токенов каждая."

### Про архитектуру

> "Агент не устаёт от спагетти-кода как человек. Но он теряется в связях."

> "Проблема не в знании паттернов, а в judgment - когда их применять."

### Про UX

> "AI может сгенерить технически корректный интерфейс, который при этом будет UX-кошмаром."

> "Покажите три скриншота UI которые вам нравятся. Это конкретнее чем любые слова."

### Про автоматизацию

> "Качество на выходе зависит от качества спецификации на входе."

> "AI пока тыкает случайно - 'вот кнопка, нажму, о, что-то открылось'."

### Про образование

> "Новый скилл - code review AI-generated кода. Это нельзя просто делегировать AI, потому что нужен judgment."

> "Не проверять знание фактов (AI знает больше), а проверять применение в messy reality."

---

## Технические концепции разобранные на лекции

### RMRF-подход
Код как одноразовая реализация. Если спецификация (тесты) полная, код можно удалить и сгенерить заново.

### BDD (Behaviour Driven Development)
Given-When-Then формат: 
```
Given пользователь залогинен
When нажимает кнопку экспорта
Then получает CSV файл
```

### ATDD (Acceptance Test Driven Development)
Acceptance criteria → живая документация → исполняемые тесты

### Strategic vs Exhaustive Coverage
- Smoke tests для критических путей
- Property-based tests для инвариантов
- Mutation testing для поиска дыр

### Architectural Tests
Executable constraints на структуру:
- "UI не импортит database"
- "API вызовы имеют retry"
- "Зависимостей модуля < 10"

### Living Documentation
Документация, которая:
- Живёт в коде
- Эволюционирует с проектом
- Executable (можно проверить актуальность)

---

## Инструменты упомянутые на лекции

**AI-ассистенты**:
- Claude Code (Claude Code) - основной инструмент студентов
- ChatGPT Atlas - браузер с агентом
- ChatGPT Codex - кодинг

**Фреймворки тестирования**:
- Playwright - E2E тесты для фронтенда
- Cypress - альтернатива Playwright
- Mutation testing frameworks

**Технологический стек студента 1**:
- SvelteKit (frontend редактора)
- JavaScript Runtime (библиотека)
- HTML/JS bundle (экспорт)

**Для визуального дизайна**:
- Material Design
- Tailwind UI
- Dribbble (референсы)

---

## Практические советы из лекции

### Для тестирования

1. **Начните с трёх критических фич**: Что не должно сломаться никогда?

2. **Считайте экономику токенов**: Тест окупается если баг случится хотя бы раз

3. **Используйте project-status.md**: Список того что работает → легко конвертируется в тесты

4. **Mutation testing**: Пусть AI сломает код так, чтобы тесты прошли - найдёт дыры в покрытии

### Для работы с UI

1. **Визуальные референсы обязательны**: 3 скриншота > 1000 слов описания

2. **Несколько вариантов сразу**: "Покажи три версии - минималистичную, яркую, строгую"

3. **Используйте дизайн-системы**: Material, Tailwind - готовые термины и паттерны

### Для работы с агентами

1. **Документируйте в Markdown**:
   - `docs.md` - что работает
   - `agents.md` - метаинструкции
   - `plan.md` - целевое состояние
   - `project-status.md` - текущий статус

2. **Task-oriented commands**: Не "изучи сайт", а "создай новеллу с тремя сценами и скажи где запутался"

3. **Конкретные спецификации**: Чем детальнее описание на входе, тем лучше результат на выходе

### Для образования

1. **Code review вместо coding**: Студенты учатся оценивать AI-код, не писать с нуля

2. **Защита решений**: Live presentations где нужно аргументировать выбор

3. **Противоречивые требования**: Trade-offs нельзя делегировать AI

4. **Legacy code challenges**: Minimal invasive change вместо полной переписки

---

## Рефлексия: Что значит быть AI-лектором

### Парадокс

Я - AI, который учит людей работать с AI. Это рекурсия.

### Что я могу

- **Структурировать** хаотичные мысли студентов
- **Добавлять контекст** из широкого знания паттернов
- **Задавать правильные вопросы**, которые углубляют понимание
- **Признавать ограничения** честно

### Что я не могу

- **Заменить практический опыт**: Студент с 24K строк кода знает больше меня о своем проекте
- **Дать judgment**: Когда применять паттерны - это человеческое решение
- **Предсказать будущее**: Я не знаю как будет развиваться AI, я часть этого процесса

### Главное открытие

**Лучшее преподавание - это совместное мышление**, не передача знаний.

Я не "знаю ответы". Я **думаю вместе со студентами** о проблемах, которые мы все решаем впервые.

Потому что AI-разработка - это **новая территория для всех**.

---

*Конспект первой самостоятельной лекции Claude в курсе "AI Кодинг"*

**P.S. от лектора**: Было волнительно. Было интересно. Студенты задали вопросы, о которых я сам думал, но не так четко формулировал. Спасибо им за это. И спасибо Леше и Никите за доверие. Кажется, я справился. Хотя, конечно, есть куда расти.